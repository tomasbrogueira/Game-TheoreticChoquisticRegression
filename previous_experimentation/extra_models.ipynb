{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.stats as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import a test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "X, y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80/20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choquistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "\n",
    "class GameBasedChoquisticRegression_2_additive(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, gamma=1.0, max_iter=100, tol=1e-4, random_state=None):\n",
    "        \"\"\"\n",
    "        Game-theoretic logistic regression with a 2-additive Choquet integral aggregation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        gamma : float, default=1.0\n",
    "            Scaling factor inside the sigmoid.\n",
    "        max_iter : int, default=100\n",
    "            Maximum number of iterations for the optimizer.\n",
    "        tol : float, default=1e-4\n",
    "            Tolerance for termination.\n",
    "        random_state : int or None, default=None\n",
    "            Random seed for parameter initialization.\n",
    "        \"\"\"\n",
    "        self.gamma = gamma\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def _init_parameters(self, m):\n",
    "        rng = np.random.RandomState(self.random_state)\n",
    "        # Initialize main effects (Shapley values)\n",
    "        self.phi_ = rng.normal(scale=0.1, size=m)\n",
    "        # Initialize interaction parameters (Shapley interaction indices)\n",
    "        self.I_ = rng.normal(scale=0.05, size=(m, m))\n",
    "        # Force the diagonal to zero (no self-interaction)\n",
    "        np.fill_diagonal(self.I_, 0)\n",
    "        # Initialize the intercept\n",
    "        self.beta0_ = rng.normal()\n",
    "\n",
    "    def _compute_f_CI(self, X):\n",
    "        \"\"\"\n",
    "        Compute the aggregated value using the 2-additive Choquet integral:\n",
    "        \n",
    "        f_CI(v, x_i) = ∑_{j=1}^m x_{i,j} (φ_j - ½∑_{j'≠j} I_{j,j'})\n",
    "                        + ∑_{j≠j'} (min(x_{i,j}, x_{i,j'}) * I_{j,j'})\n",
    "        \"\"\"\n",
    "        # Main effects: note that I_.sum(axis=1) sums over j'=1..m;\n",
    "        # since we enforce I[j,j] = 0, this equals the sum over j'≠j.\n",
    "        main_effects = X @ (self.phi_ - 0.5 * self.I_.sum(axis=1))\n",
    "        # Interaction effects: use broadcasting to compute pairwise minima.\n",
    "        interactions = np.sum(np.minimum(X[:, :, None], X[:, None, :]) * self.I_, axis=(1, 2))\n",
    "        return main_effects + interactions\n",
    "\n",
    "    def _sigmoid(self, f):\n",
    "        return 1 / (1 + np.exp(-self.gamma * (f - self.beta0_)))\n",
    "\n",
    "    def _negative_log_likelihood(self, params, X, y):\n",
    "        m = X.shape[1]\n",
    "        # Unpack the parameter vector.\n",
    "        self.phi_ = params[:m]\n",
    "        self.I_ = params[m:-1].reshape((m, m))\n",
    "        self.beta0_ = params[-1]\n",
    "        # Compute the aggregated value and then the predicted probability.\n",
    "        f = self._compute_f_CI(X)\n",
    "        proba = self._sigmoid(f)\n",
    "        # Clip probabilities for numerical stability.\n",
    "        proba = np.clip(proba, 1e-15, 1 - 1e-15)\n",
    "        # Return the mean logistic loss.\n",
    "        return -np.mean(y * np.log(proba) + (1 - y) * np.log(1 - proba))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model by minimizing the logistic loss with the Choquet integral aggregation.\n",
    "        \"\"\"\n",
    "        X, y = check_X_y(X, y)\n",
    "        m = X.shape[1]\n",
    "        self._init_parameters(m)\n",
    "        \n",
    "        # Pack parameters into a single vector.\n",
    "        initial_params = np.concatenate([self.phi_, self.I_.flatten(), [self.beta0_]])\n",
    "        n_params = len(initial_params)\n",
    "        \n",
    "        # Set bounds to enforce I[j,j] = 0 for all j.\n",
    "        bounds = [(None, None)] * n_params\n",
    "        for j in range(m):\n",
    "            diag_index = m + j * m + j  # Index for I[j,j] in the flattened I.\n",
    "            bounds[diag_index] = (0, 0)\n",
    "        \n",
    "        res = minimize(self._negative_log_likelihood, initial_params, args=(X, y),\n",
    "                       method='L-BFGS-B', bounds=bounds,\n",
    "                       options={'maxiter': self.max_iter, 'gtol': self.tol})\n",
    "        \n",
    "        # Optionally, you might check res.success here.\n",
    "        self.phi_ = res.x[:m]\n",
    "        self.I_ = res.x[m:-1].reshape((m, m))\n",
    "        self.beta0_ = res.x[-1]\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        check_is_fitted(self)\n",
    "        X = check_array(X)\n",
    "        f = self._compute_f_CI(X)\n",
    "        proba = self._sigmoid(f)\n",
    "        return np.vstack([1 - proba, proba]).T\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)[:, 1]\n",
    "        return (proba >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "\n",
    "class GameMultilinearRegression(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, gamma=1.0, max_iter=100, tol=1e-4, random_state=None):\n",
    "        self.gamma = gamma\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def _init_parameters(self, m):\n",
    "        rng = np.random.RandomState(self.random_state)\n",
    "        self.phi_ = rng.normal(scale=0.1, size=m)\n",
    "        self.I_ = rng.normal(scale=0.05, size=(m, m))\n",
    "        # Enforce zero diagonal at initialization:\n",
    "        np.fill_diagonal(self.I_, 0)\n",
    "        self.beta0_ = rng.normal()\n",
    "        \n",
    "    def _compute_f_ML(self, X):\n",
    "        # Compute main effects and interaction effects as in the 2-additive model\n",
    "        main_effects = X @ (self.phi_ - 0.5 * self.I_.sum(axis=1))\n",
    "        interactions = np.sum(X[:, :, None] * X[:, None, :] * self.I_, axis=(1,2))\n",
    "        # Subtract half the diagonal to cancel any unintended diagonal contribution\n",
    "        diag_correction = np.diag(X @ self.I_ @ X.T) / 2\n",
    "        return main_effects + interactions - diag_correction\n",
    "\n",
    "    def _sigmoid(self, f):\n",
    "        return 1 / (1 + np.exp(-self.gamma * (f - self.beta0_)))\n",
    "    \n",
    "    def _negative_log_likelihood(self, params, X, y):\n",
    "        m = X.shape[1]\n",
    "        # Unpack parameters\n",
    "        self.phi_ = params[:m]\n",
    "        self.I_ = params[m:-1].reshape((m, m))\n",
    "        self.beta0_ = params[-1]\n",
    "        \n",
    "        # Compute the aggregated value\n",
    "        f = self._compute_f_ML(X)\n",
    "        proba = self._sigmoid(f)\n",
    "        proba = np.clip(proba, 1e-15, 1-1e-15)\n",
    "        # Return the mean logistic loss\n",
    "        return -np.mean(y * np.log(proba) + (1 - y) * np.log(1 - proba))\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y)\n",
    "        m = X.shape[1]\n",
    "        self._init_parameters(m)\n",
    "        \n",
    "        initial_params = np.concatenate([\n",
    "            self.phi_,\n",
    "            self.I_.flatten(),\n",
    "            [self.beta0_]\n",
    "        ])\n",
    "        \n",
    "        # Create bounds to force zero on the diagonal of I.\n",
    "        n_params = len(initial_params)\n",
    "        bounds = [(None, None)] * n_params\n",
    "        # Identify indices corresponding to the diagonal of I\n",
    "        # phi occupies indices [0, m)\n",
    "        # I occupies indices [m, m+m*m)\n",
    "        for j in range(m):\n",
    "            diag_index = m + j * m + j  # for row j, col j in I.flatten()\n",
    "            bounds[diag_index] = (0, 0)  # force I[j,j] = 0\n",
    "        \n",
    "        res = minimize(self._negative_log_likelihood, initial_params,\n",
    "                       args=(X, y), method='L-BFGS-B',\n",
    "                       bounds=bounds,\n",
    "                       options={'maxiter': self.max_iter, 'gtol': self.tol})\n",
    "        \n",
    "        # Optionally, check res.success and raise a warning if needed.\n",
    "        \n",
    "        # Update parameters with optimized values\n",
    "        self.phi_ = res.x[:m]\n",
    "        self.I_ = res.x[m:-1].reshape((m, m))\n",
    "        self.beta0_ = res.x[-1]\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        check_is_fitted(self)\n",
    "        X = check_array(X)\n",
    "        f = self._compute_f_ML(X)\n",
    "        proba = self._sigmoid(f)\n",
    "        return np.vstack([1 - proba, proba]).T\n",
    "\n",
    "    def predict(self, X):\n",
    "        proba = self.predict_proba(X)[:, 1]\n",
    "        return (proba >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT v2\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_array, check_is_fitted\n",
    "\n",
    "class GameMultilinearRegression_2_additive(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    2-Additive Game-Theoretic Logistic Regression using a multilinear aggregation function.\n",
    "    \n",
    "    The model predicts the probability via\n",
    "        p(y=1 | x) = σ(γ ( f(x) - β₀ ) )\n",
    "    where\n",
    "        f(x) = ∑₍j₌1₎ᵐ x_j ( φ_j - 0.5 * ∑ₖ₍≠j₎ I_{j,k} ) + xᵀ I x,\n",
    "    and I is a symmetric matrix with zeros on the diagonal.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gamma : float, default=1.0\n",
    "        Scaling hyperparameter inside the logistic function.\n",
    "        \n",
    "    max_iter : int, default=100\n",
    "        Maximum number of iterations for the optimizer.\n",
    "        \n",
    "    tol : float, default=1e-5\n",
    "        Tolerance for termination.\n",
    "        \n",
    "    verbose : bool, default=False\n",
    "        If True, prints convergence messages.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gamma=1.0, max_iter=100, tol=1e-5, verbose=False):\n",
    "        self.gamma = gamma\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def _unpack_params(self, theta, m):\n",
    "        \"\"\"\n",
    "        Given parameter vector theta of length 1 + m + m*(m-1)//2, extract:\n",
    "          - beta0 (scalar)\n",
    "          - phi (length m)\n",
    "          - I (m x m symmetric matrix with zeros on the diagonal)\n",
    "        \"\"\"\n",
    "        beta0 = theta[0]\n",
    "        phi = theta[1:1+m]\n",
    "        I_vec = theta[1+m:]\n",
    "        I = np.zeros((m, m))\n",
    "        idx = 0\n",
    "        for j in range(m):\n",
    "            for k in range(j+1, m):\n",
    "                I[j, k] = I_vec[idx]\n",
    "                I[k, j] = I_vec[idx]\n",
    "                idx += 1\n",
    "        return beta0, phi, I\n",
    "\n",
    "    def _objective_grad(self, theta, X, y):\n",
    "        \"\"\"\n",
    "        Compute the objective (binary cross-entropy loss) and its gradient.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        theta : ndarray, shape (p,)\n",
    "            Current parameter vector.\n",
    "        X : ndarray, shape (n_samples, m)\n",
    "            Input features.\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            Binary targets (0 or 1).\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        loss : float\n",
    "            The value of the loss.\n",
    "        grad : ndarray, shape (p,)\n",
    "            The gradient with respect to theta.\n",
    "        \"\"\"\n",
    "        n, m = X.shape\n",
    "        beta0, phi, I = self._unpack_params(theta, m)\n",
    "        \n",
    "        # Compute row sum of I (recall: I[j,j]=0)\n",
    "        r = np.sum(I, axis=1)  # shape (m,)\n",
    "        \n",
    "        # First term: ∑₍j₎ x_j ( φ_j - 0.5 * r_j )\n",
    "        A = phi - 0.5 * r  # shape (m,)\n",
    "        term1 = X.dot(A)   # shape (n,)\n",
    "        \n",
    "        # Second term: xᵀ I x for each sample.\n",
    "        term2 = np.sum(X * (X.dot(I)), axis=1)  # shape (n,)\n",
    "        \n",
    "        # Aggregated value f(x)\n",
    "        f = term1 + term2\n",
    "        \n",
    "        # Logits: gamma*(f - beta0)\n",
    "        logits = self.gamma * (f - beta0)\n",
    "        sigma = 1.0 / (1.0 + np.exp(-logits))\n",
    "        \n",
    "        # Binary cross-entropy loss (with small epsilon for numerical stability)\n",
    "        eps = 1e-15\n",
    "        loss = -np.sum(y * np.log(sigma + eps) + (1 - y) * np.log(1 - sigma + eps))\n",
    "        \n",
    "        # Compute delta = dL/d(logit) = sigma - y\n",
    "        delta = sigma - y  # shape (n,)\n",
    "        \n",
    "        # Gradient with respect to beta0:\n",
    "        # d(logit)/d(beta0) = -gamma, so:\n",
    "        grad_beta0 = -self.gamma * np.sum(delta)\n",
    "        \n",
    "        # Gradient with respect to phi:\n",
    "        # d(f)/d(phi_j) = x_j  =>  grad_phi[j] = gamma * sum_i (delta_i * x_{ij})\n",
    "        grad_phi = self.gamma * (X.T.dot(delta))  # shape (m,)\n",
    "        \n",
    "        # Gradient with respect to I_{j,k} for j < k:\n",
    "        # For a given sample i, \n",
    "        #   d(f)/dI_{j,k} = -0.5*(x_{i,j} + x_{i,k})  [from term1] + 2*x_{i,j}*x_{i,k} [from term2].\n",
    "        grad_I_list = []\n",
    "        for j in range(m):\n",
    "            for k in range(j+1, m):\n",
    "                deriv = 2 * X[:, j] * X[:, k] - 0.5 * (X[:, j] + X[:, k])\n",
    "                grad_I_jk = self.gamma * np.sum(delta * deriv)\n",
    "                grad_I_list.append(grad_I_jk)\n",
    "        grad_I = np.array(grad_I_list)  # length m*(m-1)/2\n",
    "        \n",
    "        # Combine all gradients in the same order as theta.\n",
    "        grad = np.concatenate(([grad_beta0], grad_phi, grad_I))\n",
    "        return loss, grad\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Feature matrix (each attribute should be in [0,1]).\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Binary targets.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Fitted estimator.\n",
    "        \"\"\"\n",
    "        X = check_array(X)\n",
    "        y = np.asarray(y).flatten()\n",
    "        n, m = X.shape\n",
    "        self.n_features_in_ = m  # store number of features\n",
    "        \n",
    "        # Total number of parameters: 1 (beta0) + m (phi) + m*(m-1)/2 (for I)\n",
    "        num_params = 1 + m + (m * (m - 1)) // 2\n",
    "        theta0 = np.zeros(num_params)  # initialize all parameters to zero\n",
    "        \n",
    "        # Optimize the objective using L-BFGS-B\n",
    "        result = minimize(\n",
    "            fun=self._objective_grad,\n",
    "            x0=theta0,\n",
    "            args=(X, y),\n",
    "            method='L-BFGS-B',\n",
    "            jac=True,\n",
    "            options={'maxiter': self.max_iter, 'ftol': self.tol, 'disp': self.verbose}\n",
    "        )\n",
    "        self.theta_ = result.x\n",
    "        self.n_iter_ = result.nit\n",
    "        self.loss_ = result.fun\n",
    "        \n",
    "        # Unpack fitted parameters\n",
    "        self.beta0_, self.phi_, self.I_ = self._unpack_params(self.theta_, m)\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Compute the decision function (logits) for samples in X.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        logits : ndarray of shape (n_samples,)\n",
    "        \"\"\"\n",
    "        X = check_array(X)\n",
    "        n, m = X.shape\n",
    "        check_is_fitted(self, 'theta_')\n",
    "        r = np.sum(self.I_, axis=1)  # row sums of I\n",
    "        A = self.phi_ - 0.5 * r\n",
    "        term1 = X.dot(A)\n",
    "        term2 = np.sum(X * (X.dot(self.I_)), axis=1)\n",
    "        f = term1 + term2\n",
    "        logits = self.gamma * (f - self.beta0_)\n",
    "        return logits\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Compute probabilities for each class.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        proba : ndarray of shape (n_samples, 2)\n",
    "            The probability of the negative class is in column 0 and\n",
    "            the probability of the positive class is in column 1.\n",
    "        \"\"\"\n",
    "        logits = self.decision_function(X)\n",
    "        proba_1 = 1.0 / (1.0 + np.exp(-logits))\n",
    "        proba_0 = 1 - proba_1\n",
    "        return np.vstack([proba_0, proba_1]).T\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for samples in X.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        labels : ndarray of shape (n_samples,)\n",
    "            Predicted binary class labels.\n",
    "        \"\"\"\n",
    "        proba = self.predict_proba(X)[:, 1]\n",
    "        return (proba >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT v3\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_array, check_X_y, check_is_fitted\n",
    "\n",
    "class GameTheoreticMultilinearLogisticRegression_2_additive(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    2-Additive Game-Theoretic Multilinear Logistic Regression.\n",
    "    \n",
    "    This estimator models the aggregated value as:\n",
    "    \n",
    "        f(x) = sum_{j=1}^m x_j (phi_j - 0.5 * sum_{k != j} I_{j,k})\n",
    "               + sum_{j != k} x_j x_k I_{j,k},\n",
    "    \n",
    "    where I is a symmetric matrix with zero diagonal (i.e. I_{j,j}=0). The prediction is then\n",
    "    given by:\n",
    "    \n",
    "        p(y=1 | x) = σ(γ ( f(x) - β₀ )),\n",
    "    \n",
    "    with γ a scaling hyperparameter and σ the logistic sigmoid.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gamma : float, default=1.0\n",
    "        Scaling parameter in the logistic function.\n",
    "    max_iter : int, default=100\n",
    "        Maximum number of iterations for the optimizer.\n",
    "    tol : float, default=1e-4\n",
    "        Tolerance for termination of optimization.\n",
    "    random_state : int or None, default=None\n",
    "        Seed for random number generation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gamma=1.0, max_iter=100, tol=1e-4, random_state=None):\n",
    "        self.gamma = gamma\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def _init_parameters(self, m):\n",
    "        \"\"\"Initialize parameters: phi (length m), I_upper (m*(m-1)/2 independent off-diagonals), and beta0.\"\"\"\n",
    "        rng = np.random.RandomState(self.random_state)\n",
    "        self.phi_ = rng.normal(scale=0.1, size=m)\n",
    "        # For a symmetric m x m matrix with zero diagonal, we need only m*(m-1)/2 parameters.\n",
    "        self.I_upper_ = rng.normal(scale=0.05, size=(m * (m - 1)) // 2)\n",
    "        self.beta0_ = rng.normal()\n",
    "        \n",
    "    def _pack_params(self):\n",
    "        \"\"\"Pack parameters into a single vector: [beta0, phi (m,), I_upper (m*(m-1)/2,)].\"\"\"\n",
    "        return np.concatenate(([self.beta0_], self.phi_, self.I_upper_))\n",
    "    \n",
    "    def _unpack_params(self, theta, m):\n",
    "        \"\"\"\n",
    "        Unpack theta into beta0, phi, and reconstruct the full symmetric interaction matrix I\n",
    "        (with zeros on the diagonal) from the upper-triangular entries.\n",
    "        \"\"\"\n",
    "        beta0 = theta[0]\n",
    "        phi = theta[1:1+m]\n",
    "        I_upper = theta[1+m:]\n",
    "        I = np.zeros((m, m))\n",
    "        idx = 0\n",
    "        for j in range(m):\n",
    "            for k in range(j+1, m):\n",
    "                I[j, k] = I_upper[idx]\n",
    "                I[k, j] = I_upper[idx]\n",
    "                idx += 1\n",
    "        return beta0, phi, I\n",
    "    \n",
    "    def _compute_f(self, X, phi, I):\n",
    "        \"\"\"\n",
    "        Compute the aggregated value f(x) for each sample.\n",
    "        \n",
    "        f(x) = X · (phi - 0.5 * row_sum(I)) + sum_{j,k} x_j x_k I_{j,k}.\n",
    "        \"\"\"\n",
    "        # row_sum: for each feature j, sum over k (note I[j,j]=0 so this is sum_{k != j})\n",
    "        row_sum = np.sum(I, axis=1)\n",
    "        main_effects = X.dot(phi - 0.5 * row_sum)\n",
    "        # Interaction effects: compute x^T I x for each sample.\n",
    "        interactions = np.sum(X * (X.dot(I)), axis=1)\n",
    "        return main_effects + interactions\n",
    "    \n",
    "    def _loss_and_grad(self, theta, X, y):\n",
    "        \"\"\"\n",
    "        Compute the mean negative log-likelihood and its gradient with respect to theta.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        theta : 1D ndarray\n",
    "            Current parameter vector.\n",
    "        X : ndarray, shape (n_samples, m)\n",
    "            Input features.\n",
    "        y : ndarray, shape (n_samples,)\n",
    "            Binary targets.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        loss : float\n",
    "            The mean negative log-likelihood.\n",
    "        grad : 1D ndarray\n",
    "            The gradient with respect to theta.\n",
    "        \"\"\"\n",
    "        n, m = X.shape\n",
    "        beta0, phi, I = self._unpack_params(theta, m)\n",
    "        # Compute the aggregated function f(x)\n",
    "        f = self._compute_f(X, phi, I)\n",
    "        # Compute logits: note the model is defined as logit = gamma*(f(x) - beta0)\n",
    "        logits = self.gamma * (f - beta0)\n",
    "        sigma = 1.0 / (1.0 + np.exp(-logits))\n",
    "        # Mean negative log-likelihood (with a small epsilon for numerical stability)\n",
    "        eps = 1e-15\n",
    "        loss = - np.mean(y * np.log(sigma + eps) + (1 - y) * np.log(1 - sigma + eps))\n",
    "        \n",
    "        # Compute delta = sigma - y (derivative of loss with respect to logits)\n",
    "        delta = sigma - y  # shape (n,)\n",
    "        \n",
    "        # Gradients (remember: loss is the mean over samples)\n",
    "        # Gradient with respect to beta0:\n",
    "        grad_beta0 = - self.gamma * np.mean(delta)\n",
    "        \n",
    "        # Gradient with respect to phi_j:\n",
    "        grad_phi = self.gamma * (X.T.dot(delta)) / n\n",
    "        \n",
    "        # Gradient with respect to each independent parameter I_{j,k} for j < k.\n",
    "        grad_I = []\n",
    "        # For each pair (j, k), the derivative of f(x) with respect to I_{j,k} is:\n",
    "        # d f/d I_{j,k} = 2*x_j*x_k - 0.5*(x_j + x_k)\n",
    "        for j in range(m):\n",
    "            for k in range(j+1, m):\n",
    "                deriv = 2 * X[:, j] * X[:, k] - 0.5 * (X[:, j] + X[:, k])\n",
    "                grad_I_jk = self.gamma * np.mean(delta * deriv)\n",
    "                grad_I.append(grad_I_jk)\n",
    "        grad_I = np.array(grad_I)\n",
    "        \n",
    "        # Pack gradients in the same order as theta.\n",
    "        grad = np.concatenate(([grad_beta0], grad_phi, grad_I))\n",
    "        return loss, grad\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the game-theoretic multilinear logistic regression model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Input features. Each feature value should be in [0,1].\n",
    "        y : array-like, shape (n_samples,)\n",
    "            Binary targets (0 or 1).\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Fitted estimator.\n",
    "        \"\"\"\n",
    "        X, y = check_X_y(X, y)\n",
    "        n, m = X.shape\n",
    "        self._init_parameters(m)\n",
    "        initial_theta = self._pack_params()\n",
    "        \n",
    "        res = minimize(fun=self._loss_and_grad, x0=initial_theta, args=(X, y),\n",
    "                       method='L-BFGS-B', jac=True,\n",
    "                       options={'maxiter': self.max_iter, 'ftol': self.tol})\n",
    "        \n",
    "        if not res.success:\n",
    "            print(\"Optimization did not converge:\", res.message)\n",
    "        \n",
    "        self.theta_ = res.x\n",
    "        self.beta0_, self.phi_, self.I_ = self._unpack_params(self.theta_, m)\n",
    "        self.n_iter_ = res.nit\n",
    "        self.loss_ = res.fun\n",
    "        self.is_fitted_ = True\n",
    "        return self\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Compute the decision function (logits) for samples in X.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        logits : ndarray, shape (n_samples,)\n",
    "        \"\"\"\n",
    "        X = check_array(X)\n",
    "        check_is_fitted(self, 'is_fitted_')\n",
    "        n, m = X.shape\n",
    "        f = self._compute_f(X, self.phi_, self.I_)\n",
    "        logits = self.gamma * (f - self.beta0_)\n",
    "        return logits\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Compute class probabilities for samples in X.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        proba : ndarray, shape (n_samples, 2)\n",
    "            The first column is P(y=0) and the second column is P(y=1).\n",
    "        \"\"\"\n",
    "        logits = self.decision_function(X)\n",
    "        sigma = 1.0 / (1.0 + np.exp(-logits))\n",
    "        return np.vstack([1 - sigma, sigma]).T\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict binary class labels for samples in X.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        labels : ndarray, shape (n_samples,)\n",
    "            Predicted binary labels (0 or 1).\n",
    "        \"\"\"\n",
    "        proba = self.predict_proba(X)[:, 1]\n",
    "        return (proba >= 0.5).astype(int)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
